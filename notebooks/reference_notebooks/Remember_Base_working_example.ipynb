{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from EncoderDecoder import *\n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from LearningDynamics import *\n",
    "\n",
    "#from Beam import translate_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeMem(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_layers, heads, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.embed = Embedder(vocab_size, emb_dim)\n",
    "        self.pe = PositionalEncoder(emb_dim, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(emb_dim, heads, dropout), n_layers)\n",
    "        self.norm = Norm(emb_dim)\n",
    "        \n",
    "        self.memory = None\n",
    "        \n",
    "    def forward(self, source_sequence, source_mask):\n",
    "        '''\n",
    "        input:\n",
    "            source_sequence (sequence of source tokens) of shape (batch size, sequence length)\n",
    "            source_mask (mask over input sequence) of shape (batch size, 1, sequence length)\n",
    "        output: \n",
    "            sequence of vectors after embedding, postional encoding, attention and normalization\n",
    "            shape (batch size, sequence length, embedding dimensions)\n",
    "        '''\n",
    "        if isinstance(self.memory, torch.Tensor):\n",
    "            self.source_sequence = torch.cat([source_sequence, self.memory], dim=-1)\n",
    "        else:\n",
    "            self.source_sequence = source_sequence\n",
    "            \n",
    "        self.memory = source_sequence\n",
    "        vector_sequence = self.embed(self.source_sequence)    \n",
    "        vector_sequence = self.pe(vector_sequence)\n",
    "        source_mask = (self.source_sequence != -1).unsqueeze(-2)\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            vector_sequence = self.layers[i](vector_sequence, source_mask)\n",
    "            \n",
    "        vector_sequence = self.norm(vector_sequence)\n",
    "        \n",
    "        return vector_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        \n",
    "        self.encoder = EncodeMem(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "        \n",
    "    def repackage_hidden(self, h):\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        elif h == None:\n",
    "            return None\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        self.repackage_hidden(self.encoder.memory)\n",
    "        in_encoded = self.encoder(in_toks, in_mask)\n",
    "        in_mask = (self.encoder.source_sequence != -1).unsqueeze(-2)\n",
    "        self.d_output = self.decoder(out_toks, out_mask, in_encoded, in_mask)\n",
    "        output = self.out(self.d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_model(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    encoding = model.encoder(input_sequence, input_mask)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    \n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        input_mask = (model.encoder.source_sequence != -1).unsqueeze(-2)\n",
    "        #print(decoder_input.shape, decoder_input_mask.shape, encoding.shape, input_mask.shape)\n",
    "        out = model.out(model.decoder(decoder_input, decoder_input_mask, encoding, input_mask))\n",
    "        softout = F.softmax(out, dim=-1) \n",
    "\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0) # sample from that distribution to get next token\n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1) \n",
    "\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0][1:-1]])\n",
    "            return de_str\n",
    "        \n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi bobo !\n",
      "you are fluffy pillow\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=20, lr=0.001, \n",
    "              max_len = 25, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout = 8, 2, 4, 1, 0.01 \n",
    "chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), \n",
    "                          emb_dim, n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "load_subset_weights(chloe, opt)\n",
    "print(talk_to_model(\"my name is bobo\", chloe, opt, infield, outfield))\n",
    "print(talk_to_model(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >     >  <unk>\n",
      " >   my name is chloe  >  hi chloe !\n",
      " >   what is my name?   >  you are chloe\n",
      " >   my name is fluffy   >  hey fluffy !\n",
      " >   what is my name?   >  fluffy pillow\n",
      " >   my name is snuggles  >  hello snuggles !\n",
      " >   what is my name?   >  snuggles the bunny\n",
      " >   my name is bobo   >  hi bobo !\n",
      " >   what is my name?   >  you are bobo\n",
      " >     >  <unk> ? are chloe\n"
     ]
    }
   ],
   "source": [
    "#scheduler = CosineWithRestarts(optimizer, T_max=len(conversation_list))\n",
    "load_subset_weights(chloe, opt)\n",
    "chloe.eval()\n",
    "\n",
    "test_list = [\n",
    "    \" \",\n",
    "    \" my name is chloe\",\n",
    "    \" what is my name? \",\n",
    "    \" my name is fluffy \",\n",
    "    \" what is my name? \",\n",
    "    \" my name is snuggles\",\n",
    "    \" what is my name? \",\n",
    "    \" my name is bobo \",\n",
    "    \" what is my name? \",\n",
    "    \" \",\n",
    "]\n",
    "\n",
    "opt.k = 10\n",
    "\n",
    "for i in test_list:\n",
    "    print(\" > \", i, \" > \",  talk_to_model(i,chloe,opt,infield,outfield))\n",
    "    #chloe.update_memory() # Update Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 1.321\n",
      "0m: epoch 1 loss = 0.715\n",
      "0m: epoch 2 loss = 0.684\n",
      "0m: epoch 3 loss = 0.632\n",
      "0m: epoch 4 loss = 0.541\n",
      "0m: epoch 5 loss = 0.376\n",
      "0m: epoch 8 loss = 0.362\n",
      "0m: epoch 10 loss = 0.285\n",
      "0m: epoch 11 loss = 0.232\n",
      "0m: epoch 13 loss = 0.219\n",
      "0m: epoch 16 loss = 0.183\n",
      "0m: epoch 18 loss = 0.156\n",
      "0m: epoch 20 loss = 0.140\n",
      "0m: epoch 34 loss = 0.098\n",
      "0m: epoch 36 loss = 0.063\n",
      "0m: epoch 37 loss = 0.057\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation_list = [\n",
    "{\"listen\":\" \", \"reply\":\"so\"},\n",
    "{\"listen\":\"my name is chloe\", \"reply\":\"hi chloe!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"you are chloe\"},\n",
    "{\"listen\":\"my name is fluffy\", \"reply\":\"hey fluffy!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"fluffy pillow\"},\n",
    "{\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"snuggles the bunny\"},\n",
    "{\"listen\":\"my name is bobo\", \"reply\":\"hi bobo!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"you are bobo\"},\n",
    "                    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "opt.epochs = 50 \n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1).contiguous().view(-1)\n",
    "        \n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        \n",
    "        #chloe.update_memory() # Update Memory\n",
    "        \n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = F.cross_entropy(flat_logits, reply_labels, ignore_index = opt.trg_pad)\n",
    "\n",
    "        batch_loss.backward() #batch_loss.backward(retain_graph=True) #\n",
    "        torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, \n",
    "                                        epoch, epoch_loss))\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
